{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio Space + Dynamic Quantization with ONNX Runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gradio onnx onnxruntime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import onnx\n",
    "import gradio as gr\n",
    "\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "\n",
    "def customize_quantize_onnx_model(\n",
    "    onnx_model_path,\n",
    "    weight_type,\n",
    "    optimize_model,\n",
    "    save_directory,\n",
    "    calibration_directory=None,\n",
    "):\n",
    "    onnx_model_path = onnx_model_path.name\n",
    "    quantized_model_path = os.path.join(\n",
    "        save_directory,\n",
    "        f\"{os.path.splitext(os.path.basename(onnx_model_path))[0]}-quantized.onnx\",\n",
    "    )\n",
    "    onnx_opt_model = onnx.load(onnx_model_path)\n",
    "\n",
    "    print(\"Quantizing...\")\n",
    "\n",
    "    if calibration_directory:\n",
    "        quantize_dynamic(\n",
    "            onnx_model_path,\n",
    "            quantized_model_path,\n",
    "            weight_type=weight_type,\n",
    "            optimize_model=optimize_model,\n",
    "            calibration_path=calibration_directory,\n",
    "        )\n",
    "    else:\n",
    "        quantize_dynamic(\n",
    "            onnx_model_path,\n",
    "            quantized_model_path,\n",
    "            weight_type=weight_type,\n",
    "            optimize_model=optimize_model,\n",
    "        )\n",
    "\n",
    "    print(f\"Quantization completed. Quantized model saved to: {quantized_model_path}\")\n",
    "\n",
    "    full_precision_size = os.path.getsize(onnx_model_path) / (1024 * 1024)\n",
    "    quantized_size = os.path.getsize(quantized_model_path) / (1024 * 1024)\n",
    "\n",
    "    return (\n",
    "        f\"ONNX full precision model size (MB): {full_precision_size}\",\n",
    "        f\"ONNX quantized model size (MB): {quantized_size}\",\n",
    "        f\"ONNX quantized model saved to: {quantized_model_path}\",\n",
    "    )\n",
    "\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=customize_quantize_onnx_model,\n",
    "    inputs=[\n",
    "        gr.inputs.File(label=\"Upload ONNX Model\"),\n",
    "        gr.inputs.Dropdown(\n",
    "            [\"QInt8\", \"QUInt8\", \"QInt16\", \"QUInt16\"], label=\"Weight Type\"\n",
    "        ),\n",
    "        gr.inputs.Checkbox(label=\"Optimize Model\"),\n",
    "        gr.inputs.Textbox(label=\"Save Directory\", lines=1),\n",
    "        gr.inputs.Textbox(label=\"Calibration Data Directory (optional)\", lines=1),\n",
    "    ],\n",
    "    outputs=[\"text\", \"text\"],\n",
    "    interpretation=\"default\",\n",
    "    title=\"ORT Dynamics Quantization Space\",\n",
    "    description=\"Quantize ONNX models to lower precision using ONNX Runtime + Gradio. This space supports dynamic quantization, which quantizes the model weights at runtime, as well as static quantization, which requires calibration data for precise quantization. Select the weight type (INT8 or INT16), choose whether to optimize the model, and specify the directory to save the quantized model. For static quantization, calibration data will be saved in a separate directory within the specified save directory.\",\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
